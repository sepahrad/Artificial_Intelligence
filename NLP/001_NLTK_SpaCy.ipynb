{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### I watched and practiced Reza Shokrzad NLP course\n",
        "##### https://www.youtube.com/live/lDCoqQSc4ZE\n",
        "##### https://drive.google.com/file/d/13yHF8QW2jw3WgDcYcGaUO8Tb5TGcOY5p/view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-XdRx7uJm0v"
      },
      "source": [
        "# NLP YouTube Workshop (Session 1)\n",
        "\n",
        "Welcome to the first session of our NLP Workshop Notebook! In this introductory module, we'll lay the groundwork for understanding and applying Natural Language Processing techniques using Python's NLTK and spaCy packages. ðŸ¤“\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB_U7m8FJk5Q"
      },
      "source": [
        "## 1. NLP Toolkit\n",
        "1. **[NLTK (Natural Language Toolkit)](https://en.wikipedia.org/wiki/Natural_Language_Toolkit)** is a comprehensive open-source Python library designed for *educational* and *research* purposes in natural language processing. It provides robust tools for tasks like tokenization, stemming, lemmatization, parsing, and more, backed by extensive corpora and lexical resources. [The Natural Language Toolkit](https://www.nltk.org/)\n",
        "\n",
        "2. **[spaCy](https://en.wikipedia.org/wiki/SpaCy)** is an *industrial-strength* NLP library in Python tailored for real-world applications, emphasizing speed and accuracy in tasks such as tokenization, named entity recognition, and dependency parsing. Its modern API and efficient design make it ideal for processing large-scale text data. [spaCy Documentation](https://spacy.io/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lIYdn1woOS1n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (3.9.2)\n",
            "Requirement already satisfied: click in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from nltk) (2025.10.23)\n",
            "Requirement already satisfied: tqdm in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Sepahrad\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Sepahrad\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Sepahrad\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Sepahrad\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     C:\\Users\\Sepahrad\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\Sepahrad\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (2.3.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (2.32.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (80.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: colorama in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click>=8.0.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.2)\n",
            "Requirement already satisfied: wrapt in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in d:\\myfiles\\codes\\artificial_intelligence\\.venv\\lib\\site-packages (from jinja2->spacy) (3.0.3)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "     ----------------- ---------------------- 5.5/12.8 MB 29.2 MB/s eta 0:00:01\n",
            "     ------------------------------------ -- 12.1/12.8 MB 29.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 27.6 MB/s  0:00:00\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-3.8.0\n",
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# --- NLTK Setup ---\n",
        "!pip install nltk\n",
        "import nltk\n",
        "# Download essential NLTK datasets: tokenizers, stopwords, WordNet, etc.\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('gutenberg')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "# --- spaCy Setup ---\n",
        "!pip install spacy\n",
        "import spacy\n",
        "\n",
        "# Download and load the small English model for spaCy\n",
        "!python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubVO_GdERyVL"
      },
      "source": [
        "### **WordNet:**\n",
        "WordNet is a comprehensive lexical database within NLTK that organizes English words into synonym sets (synsets) to facilitate semantic analysis; alternatives like BabelNet or ConceptNet extend these capabilities to multilingual and broader semantic relationships.\n",
        "\n",
        "### **Gutenberg:**\n",
        "The Gutenberg corpus in NLTK is a curated collection of classic literary texts sourced from Project Gutenberg, offering diverse works for exploring historical language patterns and stylistic nuances; alternatives like the Brown Corpus or Reuters Corpus provide additional perspectives for varied text analysis.\n",
        "\n",
        "### **Punkt:**\n",
        "Punkt is an unsupervised sentence tokenizer in NLTK that intelligently segments text into sentences using learned punctuation patterns, while spaCyâ€™s built-in sentence segmentation offers a statistical alternative with robust boundary detection.\n",
        "\n",
        "### **en_core_web_sm:**\n",
        "The spaCy model en_core_web_sm is a lightweight yet efficient English model for tasks such as tokenization, POS tagging, dependency parsing, and NER; for enhanced accuracy and richer features, consider its larger counterpartsâ€”en_core_web_md, en_core_web_lg, or the transformer-based en_core_web_trf."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6fbQpjeXaxLo"
      },
      "outputs": [],
      "source": [
        "# our example in this work\n",
        "short_text = \"Natural Language Processing is fun and educational.\"\n",
        "long_text = 'Dr. Smith, traveled to Washington, D.C. on Jan. 5th for a cutting-edge NLP conference. During his keynote, he explained that advancements in tokenization techniquesâ€”particularly those implemented in NLTK and spaCy (e.g., handling abbreviations like \"Dr.\" and \"e.g.\" seamlessly)â€”are transforming text analysis.'\n",
        "text_emails = \"Contact us at admin.support_34@example.com or sales-dep@company.org for inquiries.\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGyHPx81Zqt1"
      },
      "source": [
        "## 2. Corpora and Lexical Resources\n",
        "Corpora and Lexical Resources are essential for understanding language structure and semantics. They provide extensive collections of texts (corpora) and structured word relationships (lexical databases) that support tasks like language modeling, semantic analysis, and more.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDxc7B1xaUJc",
        "outputId": "d0a52bcc-7f31-499f-af3a-58cbe75d09cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WordNet Synsets for 'computer': [Synset('great.n.01'), Synset('great.s.01'), Synset('great.s.02'), Synset('great.s.03'), Synset('bang-up.s.01'), Synset('capital.s.03'), Synset('big.s.12')]\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import wordnet as wn\n",
        "# Retrieve synsets for the word 'computer'\n",
        "synsets = wn.synsets('great')\n",
        "print(\"WordNet Synsets for 'computer':\", synsets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddu1Bb5rZqbd",
        "outputId": "f33c557e-4988-4c35-87e4-b529d08e3e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gutenberg Files: ['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import gutenberg\n",
        "# List available files in the Gutenberg corpus\n",
        "print(\"Gutenberg Files:\", gutenberg.fileids())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUjT-FsGafwM"
      },
      "source": [
        "### spaCy Lexical Resources\n",
        "While spaCy does not include separate corpora like NLTK, its language models (e.g., en_core_web_sm) incorporate rich lexical data, including vocabulary, part-of-speech tags, and dependency structures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KL3mBlBLadc4",
        "outputId": "59e52541-f9df-4240-f99e-1c0ff967e3c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token: Natural, Lemma: Natural, POS: PROPN, Dep: compound\n",
            "Token: Language, Lemma: Language, POS: PROPN, Dep: compound\n",
            "Token: Processing, Lemma: processing, POS: NOUN, Dep: nsubj\n",
            "Token: is, Lemma: be, POS: AUX, Dep: ROOT\n",
            "Token: fun, Lemma: fun, POS: ADJ, Dep: acomp\n",
            "Token: and, Lemma: and, POS: CCONJ, Dep: cc\n",
            "Token: educational, Lemma: educational, POS: ADJ, Dep: conj\n",
            "Token: ., Lemma: ., POS: PUNCT, Dep: punct\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(short_text)\n",
        "\n",
        "# Print token, lemma, POS tag, and dependency relation for each token\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, Lemma: {token.lemma_}, POS: {token.pos_}, Dep: {token.dep_}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwzdlsxxK2RJ"
      },
      "source": [
        "## 3. Tokenization Techniques\n",
        "Tokenization is the process of splitting text into smaller units such as words or sentences, which simplifies subsequent text analysis and processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xX3QUxznK5or",
        "outputId": "7a6f043d-7517-4af2-a008-64d555059b8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Tokens: ['Dr.', 'Smith', ',', 'traveled', 'to', 'Washington', ',', 'D.C.', 'on', 'Jan.', '5th', 'for', 'a', 'cutting-edge', 'NLP', 'conference', '!', 'During', 'his', 'keynote', ',', 'he', 'explained', 'that', 'advancements', 'in', 'tokenization', 'techniquesâ€”particularly', 'those', 'implemented', 'in', 'NLTK', 'and', 'spaCy', '(', 'e.g.', ',', 'handling', 'abbreviations', 'like', '``', 'Dr.', \"''\", 'and', '``', 'e.g', '.', \"''\", 'seamlessly', ')', 'â€”are', 'transforming', 'text', 'analysis', '.']\n",
            "Sentence Tokens: ['Dr. Smith, traveled to Washington, D.C. on Jan. 5th for a cutting-edge NLP conference!', 'During his keynote, he explained that advancements in tokenization techniquesâ€”particularly those implemented in NLTK and spaCy (e.g., handling abbreviations like \"Dr.\" and \"e.g.\"', 'seamlessly)â€”are transforming text analysis.']\n",
            "3\n"
          ]
        }
      ],
      "source": [
        "# NLTK Tokenization\n",
        "# Word Tokenization with NLTK\n",
        "long_text = 'Dr. Smith, traveled to Washington, D.C. on Jan. 5th for a cutting-edge NLP conference! During his keynote, he explained that advancements in tokenization techniquesâ€”particularly those implemented in NLTK and spaCy (e.g., handling abbreviations like \"Dr.\" and \"e.g.\" seamlessly)â€”are transforming text analysis.'\n",
        "\n",
        "words = nltk.word_tokenize(long_text)\n",
        "print(\"Word Tokens:\", words)\n",
        "\n",
        "# Sentence Tokenization with NLTK:\n",
        "sentences = nltk.sent_tokenize(long_text)\n",
        "print(\"Sentence Tokens:\", sentences)\n",
        "print(len(sentences))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WvUEeiKTZHc",
        "outputId": "88e69557-2858-40b5-f193-ea432dde55bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word Tokens: ['Natural', 'Language', 'Processing', 'is', 'fun', 'and', 'educational', '.']\n",
            "Sentence Tokens: ['Natural Language Processing is fun and educational.']\n"
          ]
        }
      ],
      "source": [
        "# spaCy Tokenization\n",
        "# Word Tokenization with spaCy:\n",
        "\n",
        "doc = nlp(short_text)\n",
        "words = [token.text for token in doc]\n",
        "print(\"Word Tokens:\", words)\n",
        "\n",
        "# Sentence Tokenization with spaCy\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "print(\"Sentence Tokens:\", sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVA4WIAmK4Br"
      },
      "source": [
        "## 4. Regex for Pattern Matching\n",
        "\n",
        "Regex for pattern matching is a powerful technique to extract or filter specific text patterns. With NLTK, you can leverage its RegexpTokenizer to tokenize text based on custom regex patterns, while spaCyâ€™s Matcher enables regex-like matching within its robust linguistic framework.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uXL-nbm1K59y",
        "outputId": "8e158c87-24cf-4150-f4af-bce1bbec207c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Regex Tokens: ['Hello', 'world', 'This', 'is', 'an', 'example', 'email', 'example', 'com', 'phone', '123', '456', '7890']\n"
          ]
        }
      ],
      "source": [
        "# NLTK Regex Example\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "text = \"Hello world! This is an example: email@example.com, phone: 123-456-7890.\"\n",
        "\n",
        "# Define a tokenizer that captures alphanumeric words\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"NLTK Regex Tokens:\", tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7FQfmQAbfOK",
        "outputId": "3033a4fd-80b6-4067-f378-6298fd68cb9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matched token: Hello\n",
            "Matched token: This\n",
            "Matched token: Example\n"
          ]
        }
      ],
      "source": [
        "# spaCy Regex Example\n",
        "from spacy.matcher import Matcher\n",
        "\n",
        "# Load the small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "# Define a regex-based pattern: match tokens that start with a capital letter\n",
        "pattern = [{\"TEXT\": {\"REGEX\": \"^[A-Z][a-z]+\"}}]\n",
        "matcher.add(\"CAPITAL_PATTERN\", [pattern])\n",
        "\n",
        "text = \"Hello world! This is an Example sentence.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Apply the matcher and print matched tokens\n",
        "matches = matcher(doc)\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    print(\"Matched token:\", span.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5Xl8N93bpEd",
        "outputId": "02fb7ac3-6801-4485-c6c7-d16dcf073c75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detected Emails: ['admin.support_34@example.com', 'sales-dep@company.org']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text_emails = \"Contact us at admin.support_34@example.com or sales-dep@company.org for inquiries.\"\n",
        "\n",
        "# Example 1: Email Detection\n",
        "email_pattern = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"  # Regex for email matching\n",
        "\n",
        "emails = re.findall(email_pattern, text_emails)\n",
        "print(\"Detected Emails:\", emails)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcwWc9CULCG8"
      },
      "source": [
        "## 5. Stopwords Filtering\n",
        "\n",
        "Stopwords filtering involves removing commonly used words (e.g., \"the\", \"is\", \"at\") that often add little semantic value to text analysis. This process helps focus on more meaningful terms. NLTK offers a comprehensive list of stopwords, whereas spaCy incorporates an internal attribute for each token to determine if it is a stopword."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9NfGN-QbtuK",
        "outputId": "8bfca986-abe1-409e-eb60-fa4a41fc8786"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered Words: ['simple', 'example', 'demonstrating', 'stopword', 'removal', 'natural', 'language', 'processing', '.']\n"
          ]
        }
      ],
      "source": [
        "# NLTK Stopwords Filtering\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "text = \"This is a simple example demonstrating stopword removal in natural language processing.\"\n",
        "words = nltk.word_tokenize(text)\n",
        "filtered_words = [word for word in words if word.lower() not in stopwords.words('english')]\n",
        "print(\"Filtered Words:\", filtered_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usk6SGqrK8Qw",
        "outputId": "44c2fce2-6443-466a-edc9-1f46b989b286"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered Tokens: ['Dr.', 'Smith', ',', 'traveled', 'Washington', ',', 'D.C.', 'Jan.', '5th', 'cutting', '-', 'edge', 'NLP', 'conference', '!', 'keynote', ',', 'explained', 'advancements', 'tokenization', 'techniques', 'â€”', 'particularly', 'implemented', 'NLTK', 'spaCy', '(', 'e.g.', ',', 'handling', 'abbreviations', 'like', '\"', 'Dr.', '\"', '\"', 'e.g.', '\"', 'seamlessly)â€”are', 'transforming', 'text', 'analysis', '.']\n"
          ]
        }
      ],
      "source": [
        "# spaCy Stopwords Filtering\n",
        "doc = nlp(long_text)\n",
        "filtered_tokens = [token.text for token in doc if not token.is_stop]\n",
        "print(\"Filtered Tokens:\", filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8aXW-XgK8mV"
      },
      "source": [
        "## 6. Stemming Methods\n",
        "\n",
        "Stemming reduces words to their root forms by removing affixes, thus simplifying text for analysis and retrieval tasks. NLTK offers robust stemming algorithms like Porter, Lancaster, and Snowball. While spaCy focuses on lemmatization for morphological normalization, you can integrate NLTK's stemmers with spaCy's tokenization if stemming is required."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykUhuPe_K88i",
        "outputId": "264e359b-e198-4680-ce79-5a5b164b3151"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Stemmed Words: ['run', 'runner', 'easili', 'studi', 'hardli', 'activ', 'ran']\n",
            "NLTK Snowball Words: ['run', 'runner', 'easili', 'studi', 'hard', 'activ', 'ran']\n"
          ]
        }
      ],
      "source": [
        "# NLTK Stemming Example\n",
        "from nltk.stem import PorterStemmer, SnowballStemmer\n",
        "\n",
        "# Initialize the PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "sn = SnowballStemmer(language=\"english\")\n",
        "text = \"running runner easily study hardly activity ran\"\n",
        "words = nltk.word_tokenize(text)\n",
        "stems = [ps.stem(word) for word in words]\n",
        "\n",
        "sn_stems = [sn.stem(word) for word in words]\n",
        "print(\"NLTK Stemmed Words:\", stems)\n",
        "print(\"NLTK Snowball Words:\", sn_stems)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WY4Y4EQFLERL"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy Integrated Stemmed Tokens: ['run', 'runner', 'easili', 'run']\n"
          ]
        }
      ],
      "source": [
        "# Integrating NLTK Stemming with spaCy\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Load spaCy's small English model\n",
        "ps = PorterStemmer()\n",
        "\n",
        "doc = nlp(\"running runner easily run\")\n",
        "stems = [ps.stem(token.text) for token in doc]\n",
        "print(\"spaCy Integrated Stemmed Tokens:\", stems)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seF8VOaBK_Gb"
      },
      "source": [
        "## 7. Lemmatization Strategies\n",
        "\n",
        "Lemmatization converts words into their base or dictionary forms by analyzing their context and morphology, resulting in more meaningful representations than simple stemming. NLTK uses the WordNetLemmatizer with the WordNet database, while spaCy provides built-in lemmatization integrated within its processing pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7Z7aRYQLM0m",
        "outputId": "7383c9d8-df52-41c3-f3bc-9dab5367f0c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Lemmatized Words: ['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best']\n"
          ]
        }
      ],
      "source": [
        "# NLTK Lemmatization Example\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "words = nltk.word_tokenize(text)\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "print(\"NLTK Lemmatized Words:\", lemmatized_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c69wk-IALHD5",
        "outputId": "dc8fae10-461c-4f24-ace9-93fac481c334"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy Lemmatized Tokens: ['the', 'striped', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'good']\n"
          ]
        }
      ],
      "source": [
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "doc = nlp(text)\n",
        "lemmatized_tokens = [token.lemma_ for token in doc]\n",
        "print(\"spaCy Lemmatized Tokens:\", lemmatized_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF0w487YK6Lu"
      },
      "source": [
        "## 8. Parsing and Chunking\n",
        "\n",
        "Parsing involves analyzing a sentence's grammatical structure, while chunking groups tokens into higher-level units like noun phrases. NLTK employs rule-based parsers for chunking, and spaCy utilizes statistical models to identify syntactic dependencies and extract phrase chunks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# What are all possible POS tags of NLTK?\n",
        "# https://stackoverflow.com/questions/15388831/what-are-all-possible-pos-tags-of-nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly3n8Ivgch5a",
        "outputId": "08e441cf-e0f6-4167-8d2e-23d4f2dc9e25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Parsed Tree:\n",
            "(S\n",
            "  (NP The/DT quick/JJ brown/NN)\n",
            "  (NP fox/NN)\n",
            "  jumps/VBZ\n",
            "  over/IN\n",
            "  (NP the/DT lazy/JJ dog/NN)\n",
            "  ./.)\n",
            "                                S                                          \n",
            "     ___________________________|_______________________________            \n",
            "    |        |     |            NP               NP             NP         \n",
            "    |        |     |     _______|________        |       _______|______     \n",
            "jumps/VBZ over/IN ./. The/DT quick/JJ brown/NN fox/NN the/DT lazy/JJ dog/NN\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# NLTK Parsing and Chunking Example\n",
        "# Tokenize and tag parts of speech\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "tokens = nltk.word_tokenize(text)\n",
        "tagged_tokens = nltk.pos_tag(tokens)\n",
        "\n",
        "# Define a simple chunk grammar for noun phrases (NP)\n",
        "grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "\n",
        "# Create a RegexpParser object and parse the tagged tokens\n",
        "cp = nltk.RegexpParser(grammar)\n",
        "parsed_tree = cp.parse(tagged_tokens)\n",
        "\n",
        "print(\"NLTK Parsed Tree:\")\n",
        "print(parsed_tree)\n",
        "# Uncomment the line below to visualize the tree (requires GUI support)\n",
        "parsed_tree.pretty_print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZtiTHNKc6mx",
        "outputId": "a0bf0e9d-0f3e-4ee1-8138-ef04fd2bea5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spaCy Noun Chunks:\n",
            "The striped bats\n",
            "their feet\n"
          ]
        }
      ],
      "source": [
        "# Extract and print noun chunks using spaCy's built-in functionality\n",
        "print(\"spaCy Noun Chunks:\")\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYzkGYcdLHQj"
      },
      "source": [
        "## 9. Hyponyms and Hypernyms Exploration\n",
        "\n",
        "Hyponyms and hypernyms are semantic relationships that organize words hierarchicallyâ€”hyponyms denote more specific terms (e.g., \"poodle\" for \"dog\"), while hypernyms represent more general categories (e.g., \"canine\" for \"dog\"). NLTKâ€™s WordNet is a powerful resource for exploring these relationships. Although spaCy doesn't natively extract hypernyms and hyponyms, you can integrate its NLP capabilities with NLTKâ€™s WordNet for extended lexical analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9RMAvhmLMAn",
        "outputId": "d462211d-916f-4df6-a40e-88bb31132885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hypernyms of 'dog': [Synset('domestic_animal.n.01'), Synset('canine.n.02')]\n",
            "Hyponyms of 'dog': [Synset('leonberg.n.01'), Synset('newfoundland.n.01'), Synset('poodle.n.01'), Synset('corgi.n.01'), Synset('dalmatian.n.02'), Synset('cur.n.01'), Synset('pooch.n.01'), Synset('lapdog.n.01'), Synset('pug.n.01'), Synset('spitz.n.01'), Synset('griffon.n.02'), Synset('toy_dog.n.01'), Synset('basenji.n.01'), Synset('puppy.n.01'), Synset('mexican_hairless.n.01'), Synset('great_pyrenees.n.01'), Synset('working_dog.n.01'), Synset('hunting_dog.n.01')]\n"
          ]
        }
      ],
      "source": [
        "# NLTK WordNet Example\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Choose a synset for the word 'dog'\n",
        "dog_synset = wn.synset('dog.n.01')\n",
        "\n",
        "# Retrieve hypernyms (more general terms)\n",
        "hypernyms = dog_synset.hypernyms()\n",
        "print(\"Hypernyms of 'dog':\", hypernyms)\n",
        "\n",
        "# Retrieve hyponyms (more specific terms)\n",
        "hyponyms = dog_synset.hyponyms()\n",
        "print(\"Hyponyms of 'dog':\", hyponyms)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByrSkJO6LMXa",
        "outputId": "7ced143e-7df8-41b3-a5fa-023bce1a3687"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token: dog\n",
            "Hypernyms: [Synset('domestic_animal.n.01'), Synset('canine.n.02')]\n",
            "Hyponyms: [Synset('leonberg.n.01'), Synset('newfoundland.n.01'), Synset('poodle.n.01'), Synset('corgi.n.01'), Synset('dalmatian.n.02'), Synset('cur.n.01'), Synset('pooch.n.01'), Synset('lapdog.n.01'), Synset('pug.n.01'), Synset('spitz.n.01'), Synset('griffon.n.02'), Synset('toy_dog.n.01'), Synset('basenji.n.01'), Synset('puppy.n.01'), Synset('mexican_hairless.n.01'), Synset('great_pyrenees.n.01'), Synset('working_dog.n.01'), Synset('hunting_dog.n.01')]\n"
          ]
        }
      ],
      "source": [
        "# Integrating spaCy with NLTK for Lexical Exploration\n",
        "\n",
        "from nltk.corpus import wordnet as wn\n",
        "\n",
        "# Load spaCy's small English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"The dog barked at the mailman.\")\n",
        "\n",
        "# Find the token 'dog' and explore its lexical relationships using NLTK's WordNet\n",
        "for token in doc:\n",
        "    if token.text.lower() == 'dog':\n",
        "        synsets = wn.synsets(token.text, pos=wn.NOUN)\n",
        "        if synsets:\n",
        "            synset = synsets[0]\n",
        "            hypernyms = synset.hypernyms()\n",
        "            hyponyms = synset.hyponyms()\n",
        "            print(f\"Token: {token.text}\")\n",
        "            print(\"Hypernyms:\", hypernyms)\n",
        "            print(\"Hyponyms:\", hyponyms)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qckyl7jxaczJ"
      },
      "source": [
        "## Name Entity Recognition (NER)\n",
        "\n",
        "https://github.com/explosion/spaCy/discussions/9147"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAwpv_3Xagpm",
        "outputId": "36705188-a5ef-4863-a81f-8c8ba686c521"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Named Entities:\n",
            "Apple Inc. -> ORG\n",
            "OpenAI -> PERSON\n",
            "annual -> DATE\n",
            "Oscar Award -> WORK_OF_ART\n",
            "California -> GPE\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "text = \"Apple Inc. announced a new partnership with OpenAI at the annual Oscar Award event in California.\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "print(\"Named Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.text} -> {ent.label_}\")\n",
        "\n",
        "\n",
        "# ORG (Organization)\n",
        "# GPE (Geopolitical Entity)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7SooLNC6Gpn",
        "outputId": "effdd7a6-1e8e-45ed-f101-c17d5554ad16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text (First 500 characters):\n",
            " [the tragedie of hamlet by william shakespeare 1599]   actus primus. scoena prima.  enter barnardo and francisco two centinels.    barnardo. who's there?   fran. nay answer me: stand & vnfold your selfe     bar. long liue the king     fran. barnardo?   bar. he     fran. you come most carefully vpon your houre     bar. 'tis now strook twelue, get thee to bed francisco     fran. for this releefe much thankes: 'tis bitter cold, and i am sicke at heart     barn. haue you had quiet guard?   fran. not\n",
            "\n",
            "Stemmed Text (First 500 characters):\n",
            " tragedi hamlet william shakespear 1599 actu primu scoena prima enter barnardo francisco two centinel barnardo fran nay answer stand vnfold self bar long liue king fran barnardo bar fran come care vpon hour bar strook twelu get thee bed francisco fran releef much thank bitter cold sick heart barn haue quiet guard fran mous stir barn well goodnight meet horatio marcellu riual watch bid make hast enter horatio marcellu fran think hear stand hor friend ground mar dane fran giue good night mar farwel\n",
            "\n",
            "Lemmatized Text (First 500 characters):\n",
            " tragedie hamlet william shakespeare 1599 actus primus scoena prima enter barnardo francisco two centinels barnardo fran nay answer stand vnfold selfe bar long liue king fran barnardo bar fran come carefully vpon houre bar strook twelue get thee bed francisco fran releefe much thankes bitter cold sicke heart barn haue quiet guard fran mouse stirring barn well goodnight meet horatio marcellus riuals watch bid make hast enter horatio marcellus fran thinke heare stand hor friend ground mar dane fran\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import gutenberg, stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "\n",
        "# Select a text file from Gutenberg (e.g., 'shakespeare-hamlet.txt')\n",
        "file_id = \"shakespeare-hamlet.txt\"\n",
        "raw_text = gutenberg.raw(file_id)\n",
        "\n",
        "# Step 1: Text Cleaning (Removing Gutenberg Header/Footer)\n",
        "def clean_text(text):\n",
        "    lines = text.split(\"\\n\") # break (enter - new line)\n",
        "    start_idx, end_idx = 0, len(lines)\n",
        "\n",
        "    # Removing Gutenberg boilerplate (First few and last few lines)\n",
        "    for i, line in enumerate(lines):\n",
        "        if \"START OF THIS PROJECT GUTENBERG\" in line:\n",
        "            start_idx = i + 1\n",
        "        if \"END OF THIS PROJECT GUTENBERG\" in line:\n",
        "            end_idx = i\n",
        "            break\n",
        "\n",
        "    cleaned_lines = lines[start_idx:end_idx]\n",
        "    cleaned_text = \" \".join(cleaned_lines)\n",
        "    return cleaned_text\n",
        "\n",
        "text = clean_text(raw_text)\n",
        "\n",
        "# Step 2: Lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Step 3: Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Step 4: Remove Punctuation & Stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "\n",
        "# Step 5: Stemming & Lemmatization\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "# Step 6: Convert back to text\n",
        "stemmed_text = \" \".join(stemmed_tokens)\n",
        "lemmatized_text = \" \".join(lemmatized_tokens)\n",
        "\n",
        "# Output Results\n",
        "print(\"Original Text (First 500 characters):\\n\", text[:500])\n",
        "print(\"\\nStemmed Text (First 500 characters):\\n\", stemmed_text[:500])\n",
        "print(\"\\nLemmatized Text (First 500 characters):\\n\", lemmatized_text[:500])\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
